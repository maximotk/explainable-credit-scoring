{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5db5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80713187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataproject2025.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a4f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1086683 entries, 0 to 1083761\n",
      "Data columns (total 38 columns):\n",
      " #   Column                   Non-Null Count    Dtype  \n",
      "---  ------                   --------------    -----  \n",
      " 0   issue_d                  1086236 non-null  float64\n",
      " 1   loan duration            1086236 non-null  float64\n",
      " 2   annual_inc               1086236 non-null  float64\n",
      " 3   avg_cur_bal              1086236 non-null  float64\n",
      " 4   bc_open_to_buy           1086236 non-null  float64\n",
      " 5   bc_util                  1086236 non-null  float64\n",
      " 6   delinq_2yrs              1086236 non-null  float64\n",
      " 7   dti                      1086236 non-null  float64\n",
      " 8   emp_length               1086236 non-null  object \n",
      " 9   emp_title                1086236 non-null  object \n",
      " 10  fico_range_high          1086236 non-null  float64\n",
      " 11  funded_amnt              1086236 non-null  float64\n",
      " 12  grade                    1086236 non-null  object \n",
      " 13  home_ownership           1086236 non-null  object \n",
      " 14  inq_last_6mths           1086236 non-null  float64\n",
      " 15  int_rate                 1086236 non-null  float64\n",
      " 16  mo_sin_old_rev_tl_op     1086236 non-null  float64\n",
      " 17  mo_sin_rcnt_rev_tl_op    1086236 non-null  float64\n",
      " 18  mo_sin_rcnt_tl           1086236 non-null  float64\n",
      " 19  mort_acc                 1086236 non-null  float64\n",
      " 20  mths_since_recent_bc     1086236 non-null  float64\n",
      " 21  num_actv_bc_tl           1086236 non-null  float64\n",
      " 22  num_bc_tl                1086236 non-null  float64\n",
      " 23  num_il_tl                1086236 non-null  float64\n",
      " 24  num_rev_accts            1086236 non-null  float64\n",
      " 25  open_acc                 1086236 non-null  float64\n",
      " 26  pub_rec                  1086236 non-null  float64\n",
      " 27  pub_rec_bankruptcies     1086236 non-null  float64\n",
      " 28  purpose                  1086236 non-null  object \n",
      " 29  revol_bal                1086236 non-null  float64\n",
      " 30  revol_util               1086236 non-null  float64\n",
      " 31  sub_grade                1086236 non-null  object \n",
      " 32  target                   1086236 non-null  float64\n",
      " 33  tax_liens                1086236 non-null  float64\n",
      " 34  zip_code                 1086236 non-null  float64\n",
      " 35  Pct_afro_american        1086236 non-null  float64\n",
      " 36  Predictions              1086236 non-null  float64\n",
      " 37  Predicted probabilities  1086236 non-null  float64\n",
      "dtypes: float64(32), object(6)\n",
      "memory usage: 323.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810e6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Predictions', 'Predicted probabilities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b34ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan duration</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>purpose</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>target</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>Pct_afro_american</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39600.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>21564.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2 years</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>home_improvement</td>\n",
       "      <td>4136.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>7.388592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>9570.0</td>\n",
       "      <td>16473.0</td>\n",
       "      <td>53.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.87</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>36638.0</td>\n",
       "      <td>61.2</td>\n",
       "      <td>B2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>481.0</td>\n",
       "      <td>9.745456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>53306.0</td>\n",
       "      <td>13901.0</td>\n",
       "      <td>67.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.55</td>\n",
       "      <td>5 years</td>\n",
       "      <td>sales manager</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>29581.0</td>\n",
       "      <td>54.6</td>\n",
       "      <td>A3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>7.542862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>36362.0</td>\n",
       "      <td>3567.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.03</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>10805.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>B3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>6.598132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>24161.0</td>\n",
       "      <td>4853.0</td>\n",
       "      <td>74.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.13</td>\n",
       "      <td>6 years</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>27003.0</td>\n",
       "      <td>82.8</td>\n",
       "      <td>D5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>7.058900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   issue_d  loan duration  annual_inc  avg_cur_bal  bc_open_to_buy  bc_util  \\\n",
       "0   2013.0            0.0     39600.0       1379.0         21564.0     16.1   \n",
       "1   2013.0            0.0     55000.0       9570.0         16473.0     53.9   \n",
       "2   2013.0            0.0    325000.0      53306.0         13901.0     67.1   \n",
       "3   2013.0            0.0    130000.0      36362.0          3567.0     93.0   \n",
       "4   2013.0            1.0     73000.0      24161.0          4853.0     74.7   \n",
       "\n",
       "   delinq_2yrs    dti emp_length      emp_title  ...  pub_rec  \\\n",
       "0          0.0   2.49    2 years          other  ...      0.0   \n",
       "1          0.0  22.87  10+ years          other  ...      0.0   \n",
       "2          0.0  18.55    5 years  sales manager  ...      0.0   \n",
       "3          0.0  13.03  10+ years          other  ...      0.0   \n",
       "4          1.0  23.13    6 years          other  ...      0.0   \n",
       "\n",
       "   pub_rec_bankruptcies             purpose revol_bal  revol_util  sub_grade  \\\n",
       "0                   0.0    home_improvement    4136.0        16.1         B2   \n",
       "1                   0.0  debt_consolidation   36638.0        61.2         B2   \n",
       "2                   0.0  debt_consolidation   29581.0        54.6         A3   \n",
       "3                   0.0  debt_consolidation   10805.0        67.0         B3   \n",
       "4                   0.0  debt_consolidation   27003.0        82.8         D5   \n",
       "\n",
       "   target  tax_liens  zip_code  Pct_afro_american  \n",
       "0     0.0        0.0     782.0           7.388592  \n",
       "1     0.0        0.0     481.0           9.745456  \n",
       "2     0.0        0.0     945.0           7.542862  \n",
       "3     0.0        0.0     809.0           6.598132  \n",
       "4     1.0        0.0     802.0           7.058900  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2bf8154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_d                  447\n",
       "loan duration            447\n",
       "annual_inc               447\n",
       "avg_cur_bal              447\n",
       "bc_open_to_buy           447\n",
       "bc_util                  447\n",
       "delinq_2yrs              447\n",
       "dti                      447\n",
       "emp_length               447\n",
       "emp_title                447\n",
       "fico_range_high          447\n",
       "funded_amnt              447\n",
       "grade                    447\n",
       "home_ownership           447\n",
       "inq_last_6mths           447\n",
       "int_rate                 447\n",
       "mo_sin_old_rev_tl_op     447\n",
       "mo_sin_rcnt_rev_tl_op    447\n",
       "mo_sin_rcnt_tl           447\n",
       "mort_acc                 447\n",
       "mths_since_recent_bc     447\n",
       "num_actv_bc_tl           447\n",
       "num_bc_tl                447\n",
       "num_il_tl                447\n",
       "num_rev_accts            447\n",
       "open_acc                 447\n",
       "pub_rec                  447\n",
       "pub_rec_bankruptcies     447\n",
       "purpose                  447\n",
       "revol_bal                447\n",
       "revol_util               447\n",
       "sub_grade                447\n",
       "target                   447\n",
       "tax_liens                447\n",
       "zip_code                 447\n",
       "Pct_afro_american        447\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d82c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NA: 447\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan duration</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>bc_open_to_buy</th>\n",
       "      <th>bc_util</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>purpose</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>target</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>Pct_afro_american</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7060</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068750</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083761</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         issue_d  loan duration  annual_inc  avg_cur_bal  bc_open_to_buy  \\\n",
       "2276         NaN            NaN         NaN          NaN             NaN   \n",
       "5641         NaN            NaN         NaN          NaN             NaN   \n",
       "6398         NaN            NaN         NaN          NaN             NaN   \n",
       "7060         NaN            NaN         NaN          NaN             NaN   \n",
       "7100         NaN            NaN         NaN          NaN             NaN   \n",
       "...          ...            ...         ...          ...             ...   \n",
       "1068020      NaN            NaN         NaN          NaN             NaN   \n",
       "1068750      NaN            NaN         NaN          NaN             NaN   \n",
       "1069795      NaN            NaN         NaN          NaN             NaN   \n",
       "1070263      NaN            NaN         NaN          NaN             NaN   \n",
       "1083761      NaN            NaN         NaN          NaN             NaN   \n",
       "\n",
       "         bc_util  delinq_2yrs  dti emp_length emp_title  ...  pub_rec  \\\n",
       "2276         NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "5641         NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "6398         NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "7060         NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "7100         NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "...          ...          ...  ...        ...       ...  ...      ...   \n",
       "1068020      NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "1068750      NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "1069795      NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "1070263      NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "1083761      NaN          NaN  NaN        NaN       NaN  ...      NaN   \n",
       "\n",
       "         pub_rec_bankruptcies purpose revol_bal  revol_util  sub_grade  \\\n",
       "2276                      NaN     NaN       NaN         NaN        NaN   \n",
       "5641                      NaN     NaN       NaN         NaN        NaN   \n",
       "6398                      NaN     NaN       NaN         NaN        NaN   \n",
       "7060                      NaN     NaN       NaN         NaN        NaN   \n",
       "7100                      NaN     NaN       NaN         NaN        NaN   \n",
       "...                       ...     ...       ...         ...        ...   \n",
       "1068020                   NaN     NaN       NaN         NaN        NaN   \n",
       "1068750                   NaN     NaN       NaN         NaN        NaN   \n",
       "1069795                   NaN     NaN       NaN         NaN        NaN   \n",
       "1070263                   NaN     NaN       NaN         NaN        NaN   \n",
       "1083761                   NaN     NaN       NaN         NaN        NaN   \n",
       "\n",
       "         target  tax_liens  zip_code  Pct_afro_american  \n",
       "2276        NaN        NaN       NaN                NaN  \n",
       "5641        NaN        NaN       NaN                NaN  \n",
       "6398        NaN        NaN       NaN                NaN  \n",
       "7060        NaN        NaN       NaN                NaN  \n",
       "7100        NaN        NaN       NaN                NaN  \n",
       "...         ...        ...       ...                ...  \n",
       "1068020     NaN        NaN       NaN                NaN  \n",
       "1068750     NaN        NaN       NaN                NaN  \n",
       "1069795     NaN        NaN       NaN                NaN  \n",
       "1070263     NaN        NaN       NaN                NaN  \n",
       "1083761     NaN        NaN       NaN                NaN  \n",
       "\n",
       "[447 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1086236\n",
      "36        447\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na_rows = df.isna().any(axis=1)\n",
    "print(f\"Number of rows with NA: {na_rows.sum()}\")\n",
    "display(df[na_rows])\n",
    "\n",
    "print(df.isna().sum(axis=1).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65df82e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c7b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.dropna(axis=0) # either y misses or all X miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe68873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df: pd.DataFrame()) -> pd.DataFrame():\n",
    "    \"\"\"\n",
    "    Get additional features.\n",
    "    \"\"\"        \n",
    "\n",
    "    df_with_features = (\n",
    "        df\n",
    "        .assign(\n",
    "\n",
    "            # logs --> to money, not months or so\n",
    "            annual_inc_log = np.log1p(df[\"annual_inc\"]),\n",
    "            avg_cur_bal_log = np.log1p(df[\"avg_cur_bal\"]),\n",
    "            fico_range_high_log = np.log1p(df[\"fico_range_high\"]),\n",
    "            revol_bal_log = np.log1p(df[\"revol_bal\"]),\n",
    "\n",
    "            # broader zip code area\n",
    "            zip_code2 = np.round(df[\"zip_code\"] / 10, 0),\n",
    "\n",
    "            # total balance?\n",
    "            cur_balance = df[\"avg_cur_bal\"] * df[\"open_acc\"],\n",
    "\n",
    "            # flags\n",
    "            delinq_2yrs_flag = df[\"delinq_2yrs\"] >= 1,\n",
    "            tax_liens_flag = df[\"tax_liens\"] >= 1,\n",
    "\n",
    "            # shares\n",
    "            s_actv_bc_tl = df[\"num_actv_bc_tl\"] / (df[\"open_acc\"] + 1e-6),\n",
    "            s_bc_tl = df[\"num_bc_tl\"] / (df[\"open_acc\"] + 1e-6),\n",
    "            s_il_tl = df[\"num_il_tl\"] / (df[\"open_acc\"] + 1e-6),\n",
    "            s_rev_accts = df[\"num_rev_accts\"] / (df[\"open_acc\"] + 1e-6),\n",
    "\n",
    "            # interactions\n",
    "            int_rate_x_duration = df[\"int_rate\"] * df[\"loan duration\"], # higher rates are even riskier on 60 vs 36\n",
    "            dti_x_util = df[\"dti\"] * (df[\"revol_util\"] / 100.0), # debt burden (DTI) is more problematic if utilization of their cards/lines is also high\n",
    "            revol_bal_income_ratio = df[\"revol_bal\"] / (df[\"annual_inc\"] + 1e-6), # leverage: outstanding revolving balance / income\n",
    "            fico_x_dti = df[\"fico_range_high\"] * df[\"dti\"], # same DTI can mean different risk depending on FICO score; \"do they manage well or not?\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_with_features\n",
    "\n",
    "df_engineered = get_features(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d94ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"Encodings of categorical variables.\"\"\"\n",
    "\n",
    "  df_encoded = df.copy()\n",
    "\n",
    "  # grade to numeric\n",
    "  grade_map = {c: i+1 for i, c in enumerate(string.ascii_uppercase[:7])}\n",
    "  df_encoded[\"grade_num\"] = df_encoded[\"grade\"].map(grade_map)\n",
    "\n",
    "  # sub_grade to numeric\n",
    "  sg = df_encoded[\"sub_grade\"].astype(str).str.upper().str.strip()\n",
    "  letter = sg.str[0]\n",
    "  number = pd.to_numeric(sg.str[1:].str.extract(r\"(\\d+)\", expand=False), errors=\"coerce\")\n",
    "  letter_map = {ch: i+1 for i, ch in enumerate(\"ABCDEFG\")}\n",
    "  base = letter.map(letter_map)\n",
    "  sub_grade_num = (base - 1) * 5 + number\n",
    "  df_encoded[\"sub_grade_num\"] = sub_grade_num.astype(\"float32\")\n",
    "\n",
    "  # emp_length to numeric; map prob cleanest; maybe 10+ different?\n",
    "  emp_length_map = {\n",
    "    '< 1 year': 0,\n",
    "    '1 year': 1,\n",
    "    '2 years': 2,\n",
    "    '3 years': 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    '10+ years': 10\n",
    "  }\n",
    "\n",
    "  df_encoded[\"emp_length_num\"] = df_encoded[\"emp_length\"].map(emp_length_map).astype(\"float32\")\n",
    "\n",
    "  # one-hot\n",
    "  onehot_cols = [\"home_ownership\", \"purpose\", \"emp_title\"]\n",
    "  df_encoded = pd.get_dummies(df_encoded, columns=onehot_cols, prefix=onehot_cols, drop_first=True)\n",
    "\n",
    "  # drop originals\n",
    "  df_encoded = df_encoded.drop(columns=[\"grade\", \"sub_grade\", \"emp_length\"])\n",
    "\n",
    "  return df_encoded\n",
    "\n",
    "df_encoded = categorical_encoding(df_engineered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c8583",
   "metadata": {},
   "source": [
    "## STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86de3219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (868988, 112)\n",
      "Test set size: (217248, 112)\n",
      "Target distribution in training set:\n",
      "target\n",
      "0.0    0.789505\n",
      "1.0    0.210495\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "\n",
      "XGBOOST MODEL\n",
      "==================================================\n",
      "Performing cross-validation...\n",
      "XGBoost CV Scores: [0.79460615 0.7945371  0.79438794]\n",
      "XGBoost CV Mean: 0.7945 (+/- 0.0002)\n",
      "Training XGBoost on full training set...\n",
      "XGBoost Training Accuracy: 0.8006\n",
      "XGBoost Test Accuracy: 0.7951\n",
      "\n",
      "XGBoost Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.98      0.88    171518\n",
      "         1.0       0.57      0.11      0.19     45730\n",
      "\n",
      "    accuracy                           0.80    217248\n",
      "   macro avg       0.69      0.55      0.54    217248\n",
      "weighted avg       0.76      0.80      0.74    217248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_col = 'target'\n",
    "# Separate features and target\n",
    "X = df_encoded.drop(columns=[target_col])\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Target distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"XGBOOST MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Cross-validation for XGBoost\n",
    "print(\"Performing cross-validation...\")\n",
    "xgb_cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(f\"XGBoost CV Scores: {xgb_cv_scores}\")\n",
    "print(f\"XGBoost CV Mean: {xgb_cv_scores.mean():.4f} (+/- {xgb_cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Train XGBoost on full training set\n",
    "print(\"Training XGBoost on full training set...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_train_pred = xgb_model.predict(X_train)\n",
    "xgb_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_train_acc = accuracy_score(y_train, xgb_train_pred)\n",
    "xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
    "\n",
    "print(f\"XGBoost Training Accuracy: {xgb_train_acc:.4f}\")\n",
    "print(f\"XGBoost Test Accuracy: {xgb_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nXGBoost Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, xgb_test_pred))\n",
    "\n",
    "\n",
    "# # Plot confusion matrices\n",
    "# plt.subplot(2, 2, 2)\n",
    "# xgb_cm = confusion_matrix(y_test, xgb_test_pred)\n",
    "# sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('XGBoost Confusion Matrix')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.xlabel('Predicted Label')\n",
    "\n",
    "# # Feature importance comparison (top 10 features)\n",
    "# plt.subplot(2, 2, 4)\n",
    "# xgb_importance = xgb_model.feature_importances_\n",
    "# rf_importance = rf_model.feature_importances_\n",
    "\n",
    "# # Get top 10 features from XGBoost\n",
    "# top_features_idx = np.argsort(xgb_importance)[-10:]\n",
    "# top_features = X.columns[top_features_idx]\n",
    "\n",
    "# x_pos = np.arange(len(top_features))\n",
    "# plt.barh(x_pos - 0.2, xgb_importance[top_features_idx], 0.4, label='XGBoost', alpha=0.8)\n",
    "# plt.barh(x_pos + 0.2, rf_importance[top_features_idx], 0.4, label='Random Forest', alpha=0.8)\n",
    "# plt.yticks(x_pos, top_features)\n",
    "# plt.xlabel('Feature Importance')\n",
    "# plt.title('Top 10 Feature Importance Comparison')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01500bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-25 12:11:25,029] A new study created in memory with name: xgboost_optimization\n",
      "[I 2025-09-25 12:12:33,824] Trial 0 finished with value: 0.7944965871383759 and parameters: {'n_estimators': 160, 'max_depth': 9, 'learning_rate': 0.027628746285086818, 'subsample': 0.9660586550478428, 'colsample_bytree': 0.9479986597300765, 'colsample_bylevel': 0.9821966234200675, 'reg_alpha': 0.02007619755276957, 'reg_lambda': 7.515856292107318e-07, 'min_child_weight': 5}. Best is trial 0 with value: 0.7944965871383759.\n",
      "[I 2025-09-25 12:13:08,928] Trial 1 finished with value: 0.7929027796281592 and parameters: {'n_estimators': 91, 'max_depth': 10, 'learning_rate': 0.1982029689359225, 'subsample': 0.8954679668559986, 'colsample_bytree': 0.7583013737778234, 'colsample_bylevel': 0.9940873458396307, 'reg_alpha': 0.009726171007469066, 'reg_lambda': 0.05300274571711142, 'min_child_weight': 4}. Best is trial 0 with value: 0.7944965871383759.\n",
      "[I 2025-09-25 12:13:46,419] Trial 2 finished with value: 0.7947808257074217 and parameters: {'n_estimators': 112, 'max_depth': 7, 'learning_rate': 0.0732330672526568, 'subsample': 0.6688170293349013, 'colsample_bytree': 0.6697940147269584, 'colsample_bylevel': 0.9267993147623985, 'reg_alpha': 0.8735116668001693, 'reg_lambda': 2.462350573222263e-06, 'min_child_weight': 5}. Best is trial 2 with value: 0.7947808257074217.\n",
      "[I 2025-09-25 12:14:43,021] Trial 3 finished with value: 0.7940949704353669 and parameters: {'n_estimators': 180, 'max_depth': 7, 'learning_rate': 0.02531223817596731, 'subsample': 0.98957513948097, 'colsample_bytree': 0.896722196838948, 'colsample_bylevel': 0.8782971251778946, 'reg_alpha': 4.490457764709596e-08, 'reg_lambda': 0.0584362152142957, 'min_child_weight': 3}. Best is trial 2 with value: 0.7947808257074217.\n",
      "[I 2025-09-25 12:15:37,555] Trial 4 finished with value: 0.7948648312903481 and parameters: {'n_estimators': 289, 'max_depth': 5, 'learning_rate': 0.04909240990311896, 'subsample': 0.784940323037785, 'colsample_bytree': 0.8073024439073039, 'colsample_bylevel': 0.9586949640123141, 'reg_alpha': 0.01803304836129018, 'reg_lambda': 0.0031520127970603067, 'min_child_weight': 10}. Best is trial 4 with value: 0.7948648312903481.\n",
      "[I 2025-09-25 12:16:29,894] Trial 5 finished with value: 0.7953792226996955 and parameters: {'n_estimators': 196, 'max_depth': 8, 'learning_rate': 0.06727277923542496, 'subsample': 0.8817326477610854, 'colsample_bytree': 0.8053526658547998, 'colsample_bylevel': 0.9978401763464493, 'reg_alpha': 0.004891250637010883, 'reg_lambda': 0.0032177883420182168, 'min_child_weight': 9}. Best is trial 5 with value: 0.7953792226996955.\n",
      "[I 2025-09-25 12:17:19,096] Trial 6 finished with value: 0.7912065529595904 and parameters: {'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.011030653777931115, 'subsample': 0.9123580702788144, 'colsample_bytree': 0.8268929166968699, 'colsample_bylevel': 0.6187556208369384, 'reg_alpha': 0.0023804263454165605, 'reg_lambda': 0.708157724788831, 'min_child_weight': 9}. Best is trial 5 with value: 0.7953792226996955.\n",
      "[I 2025-09-25 12:19:16,355] Trial 7 finished with value: 0.7950788733357271 and parameters: {'n_estimators': 296, 'max_depth': 10, 'learning_rate': 0.019412397504844227, 'subsample': 0.9746878525622484, 'colsample_bytree': 0.6681035937399069, 'colsample_bylevel': 0.9226160235986237, 'reg_alpha': 0.004266235283094583, 'reg_lambda': 6.629775631098767e-07, 'min_child_weight': 9}. Best is trial 5 with value: 0.7953792226996955.\n",
      "[I 2025-09-25 12:19:47,362] Trial 8 finished with value: 0.7948164994697148 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.17873892521721244, 'subsample': 0.9120010546638329, 'colsample_bytree': 0.9600076341939794, 'colsample_bylevel': 0.6391015209180265, 'reg_alpha': 0.01649248876405449, 'reg_lambda': 1.3569644855948273e-06, 'min_child_weight': 10}. Best is trial 5 with value: 0.7953792226996955.\n",
      "[I 2025-09-25 12:20:37,383] Trial 9 finished with value: 0.7950708182799247 and parameters: {'n_estimators': 178, 'max_depth': 8, 'learning_rate': 0.04675305764886212, 'subsample': 0.7793867733220472, 'colsample_bytree': 0.9047619759066554, 'colsample_bylevel': 0.9183372432819914, 'reg_alpha': 0.000270593808152979, 'reg_lambda': 0.00011515783896088595, 'min_child_weight': 10}. Best is trial 5 with value: 0.7953792226996955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters: {'n_estimators': 196, 'max_depth': 8, 'learning_rate': 0.06727277923542496, 'subsample': 0.8817326477610854, 'colsample_bytree': 0.8053526658547998, 'colsample_bylevel': 0.9978401763464493, 'reg_alpha': 0.004891250637010883, 'reg_lambda': 0.0032177883420182168, 'min_child_weight': 9}\n",
      "Best CV score: 0.7954\n",
      "\n",
      "Training final optimized XGBoost model...\n",
      "Optimized XGBoost Training Accuracy: 0.8016\n",
      "Optimized XGBoost Test Accuracy: 0.7953\n",
      "\n",
      "Optimized XGBoost Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.98      0.88    171518\n",
      "         1.0       0.58      0.10      0.17     45730\n",
      "\n",
      "    accuracy                           0.80    217248\n",
      "   macro avg       0.69      0.54      0.53    217248\n",
      "weighted avg       0.76      0.80      0.73    217248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Create model with suggested parameters\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Return mean CV score (Optuna maximizes this)\n",
    "    return cv_scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='xgboost_optimization')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score: {study.best_value:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(\"\\nTraining final optimized XGBoost model...\")\n",
    "optimized_xgb = xgb.XGBClassifier(**best_params)\n",
    "optimized_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate optimized model\n",
    "optimized_train_pred = optimized_xgb.predict(X_train)\n",
    "optimized_test_pred = optimized_xgb.predict(X_test)\n",
    "\n",
    "optimized_train_acc = accuracy_score(y_train, optimized_train_pred)\n",
    "optimized_test_acc = accuracy_score(y_test, optimized_test_pred)\n",
    "\n",
    "print(f\"Optimized XGBoost Training Accuracy: {optimized_train_acc:.4f}\")\n",
    "print(f\"Optimized XGBoost Test Accuracy: {optimized_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nOptimized XGBoost Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, optimized_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e852d3",
   "metadata": {},
   "source": [
    "## STEP 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282fe7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "optimized_xgb = load('optimized_xgb_model.pkl')\n",
    "\n",
    "y_pred = optimized_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f2dfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 209170, 1: 8078}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac6d6e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['issue_d', 'loan duration', 'annual_inc', 'avg_cur_bal',\n",
      "       'bc_open_to_buy', 'bc_util', 'delinq_2yrs', 'dti', 'emp_length',\n",
      "       'emp_title', 'fico_range_high', 'funded_amnt', 'grade',\n",
      "       'home_ownership', 'inq_last_6mths', 'int_rate', 'mo_sin_old_rev_tl_op',\n",
      "       'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc',\n",
      "       'mths_since_recent_bc', 'num_actv_bc_tl', 'num_bc_tl', 'num_il_tl',\n",
      "       'num_rev_accts', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies',\n",
      "       'purpose', 'revol_bal', 'revol_util', 'sub_grade', 'target',\n",
      "       'tax_liens', 'zip_code', 'Pct_afro_american'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.086236e+06\n",
       "mean     1.290673e+01\n",
       "std      1.206943e+01\n",
       "min      4.301260e-02\n",
       "25%      3.971440e+00\n",
       "50%      8.868146e+00\n",
       "75%      1.853919e+01\n",
       "max      7.036799e+01\n",
       "Name: Pct_afro_american, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df[\"Pct_afro_american\"].head()\n",
    "df[\"Pct_afro_american\"].describe()\n",
    "\n",
    "# the only ethcnicity variable is Pct_afro_american: Average percentage of African American people living in the area covered by the three-digit ZIP code (census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a52ad284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low     109416\n",
      "High    107832\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#splitting the test set by median of this variable\n",
    "\n",
    "# Extract protected attribute for test set\n",
    "pct_afro = df.loc[X_test.index, \"Pct_afro_american\"]\n",
    "\n",
    "# Median split\n",
    "median_val = pct_afro.median()\n",
    "ethnicity_group = pd.Series(\n",
    "    [\"High\" if val > median_val else \"Low\" for val in pct_afro],\n",
    "    index=pct_afro.index\n",
    ")\n",
    "\n",
    "print(ethnicity_group.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11b7d299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         y_true  y_pred  Pct_afro_american ethnicity_group  annual_inc  \\\n",
      "199648      0.0       0           6.669588             Low    128000.0   \n",
      "465152      0.0       0           4.657994             Low    128000.0   \n",
      "1061172     0.0       0          15.194674            High     53000.0   \n",
      "356750      0.0       0           1.698836             Low     31000.0   \n",
      "485653      0.0       0           6.522108             Low     70000.0   \n",
      "\n",
      "         fico_range_high grade  loan_duration  \n",
      "199648             729.0     B            1.0  \n",
      "465152             719.0     C            1.0  \n",
      "1061172            779.0     A            0.0  \n",
      "356750             674.0     D            0.0  \n",
      "485653             674.0     D            0.0  \n",
      "ethnicity_group\n",
      "Low     109416\n",
      "High    107832\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a new df : results_df\n",
    "results_df = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"Pct_afro_american\": df.loc[X_test.index, \"Pct_afro_american\"],\n",
    "    \"ethnicity_group\": ethnicity_group,\n",
    "    \"annual_inc\": df.loc[X_test.index, \"annual_inc\"],\n",
    "    \"fico_range_high\": df.loc[X_test.index, \"fico_range_high\"],\n",
    "    \"grade\": df.loc[X_test.index, \"grade\"],\n",
    "    \"loan_duration\": df.loc[X_test.index, \"loan duration\"]   # rename to avoid space in column name\n",
    "})\n",
    "\n",
    "# ⚠️ Rename column so it has no space\n",
    "results_df = results_df.rename(columns={\"loan_duration\": \"loan_duration\"})\n",
    "\n",
    "# Quick check\n",
    "print(results_df.head())\n",
    "print(results_df[\"ethnicity_group\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c140d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demographic parity (selection rate):\n",
      " ethnicity_group\n",
      "High    0.041555\n",
      "Low     0.032875\n",
      "Name: y_pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, roc_auc_score\n",
    "\n",
    "# === 1. Statistical Parity (Demographic Parity)b\n",
    "dp = results_df.groupby(\"ethnicity_group\")[\"y_pred\"].mean()\n",
    "print(\"\\nDemographic parity (selection rate):\\n\", dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15136d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted gap (max-min across groups within strata): 0.010311004228228235\n",
      "Weighted ratio (min/max across groups within strata, valid strata only): 0.4796935594665693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/_41jr63j245210xpt44m7yjh0000gn/T/ipykernel_7075/3551137818.py:23: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  work.groupby(strata + [group_col])[yhat_col]\n",
      "/var/folders/jd/_41jr63j245210xpt44m7yjh0000gn/T/ipykernel_7075/3551137818.py:36: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  weights = work.groupby(strata).size().reindex(gaps.index).rename(\"n\")\n"
     ]
    }
   ],
   "source": [
    "# 2. Conditional statistical parity with safe ratio calculation\n",
    "def conditional_stat_parity_stratified(\n",
    "    df,\n",
    "    group_col=\"ethnicity_group\",\n",
    "    cond_spec={\"annual_inc\": \"q4\", \"fico_range_high\": \"q4\", \"grade\": \"cat\", \"loan_duration\": \"cat\"},\n",
    "    yhat_col=\"y_pred\",\n",
    "):\n",
    "    work = df[[group_col, yhat_col] + list(cond_spec.keys())].copy()\n",
    "\n",
    "    # Bin continuous variables, cast categoricals\n",
    "    for col, spec in cond_spec.items():\n",
    "        if spec.startswith(\"q\"):  # e.g. \"q4\" for quartiles\n",
    "            q = int(spec[1:])\n",
    "            work[col] = pd.qcut(work[col], q=q, duplicates=\"drop\")\n",
    "        elif spec == \"cat\":\n",
    "            work[col] = work[col].astype(\"category\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown spec for {col}: {spec}\")\n",
    "\n",
    "    # Selection rate within each stratum and group\n",
    "    strata = list(cond_spec.keys())\n",
    "    rates = (\n",
    "        work.groupby(strata + [group_col])[yhat_col]\n",
    "            .mean()\n",
    "            .unstack(group_col)\n",
    "    )\n",
    "\n",
    "    # Keep only strata where both groups are present\n",
    "    rates = rates.dropna()\n",
    "\n",
    "    # Per-stratum gap and ratio\n",
    "    gaps = (rates.max(axis=1) - rates.min(axis=1)).rename(\"gap\")\n",
    "    ratios = (rates.min(axis=1) / rates.max(axis=1)).rename(\"ratio\")\n",
    "\n",
    "    # Stratum weights (number of samples per stratum)\n",
    "    weights = work.groupby(strata).size().reindex(gaps.index).rename(\"n\")\n",
    "\n",
    "    # Weighted gap (always valid)\n",
    "    w_gap = np.average(gaps.values, weights=weights.values)\n",
    "\n",
    "    # Weighted ratio (only valid strata, drop NaNs)\n",
    "    valid_ratios = ratios.dropna()\n",
    "    if not valid_ratios.empty:\n",
    "        w_ratio = np.average(valid_ratios.values, weights=weights.loc[valid_ratios.index].values)\n",
    "    else:\n",
    "        w_ratio = np.nan\n",
    "\n",
    "    # Build summary table\n",
    "    summary = pd.DataFrame({\n",
    "        \"n\": weights,\n",
    "        \"gap\": gaps,\n",
    "        \"ratio\": ratios\n",
    "    }).sort_values(\"n\", ascending=False)\n",
    "\n",
    "    return w_gap, w_ratio, rates, summary\n",
    "\n",
    "\n",
    "# === Run it ===\n",
    "w_gap, w_ratio, rate_table, per_stratum = conditional_stat_parity_stratified(\n",
    "    results_df,\n",
    "    group_col=\"ethnicity_group\",\n",
    "    cond_spec={\n",
    "        \"annual_inc\": \"q4\",\n",
    "        \"fico_range_high\": \"q4\",\n",
    "        \"grade\": \"cat\",\n",
    "        \"loan_duration\": \"cat\"\n",
    "    },\n",
    "    yhat_col=\"y_pred\"\n",
    ")\n",
    "\n",
    "print(\"Weighted gap (max-min across groups within strata):\", w_gap)\n",
    "print(\"Weighted ratio (min/max across groups within strata, valid strata only):\", w_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9de2600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True positive rate (Equal opportunity):\n",
      " ethnicity_group\n",
      "High    0.111757\n",
      "Low     0.092008\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/_41jr63j245210xpt44m7yjh0000gn/T/ipykernel_7075/3685368568.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tpr_values = results_df.groupby(\"ethnicity_group\").apply(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3. Equal Opportunity (True Positive Rate per group)\n",
    "def tpr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "\n",
    "tpr_values = results_df.groupby(\"ethnicity_group\").apply(\n",
    "    lambda g: tpr(g[\"y_true\"], g[\"y_pred\"])\n",
    ")\n",
    "print(\"\\nTrue positive rate (Equal opportunity):\\n\", tpr_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e23c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "False positive rate (Equalized odds):\n",
      " ethnicity_group\n",
      "High    0.021919\n",
      "Low     0.017856\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/_41jr63j245210xpt44m7yjh0000gn/T/ipykernel_7075/4251086996.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  fpr_values = results_df.groupby(\"ethnicity_group\").apply(\n"
     ]
    }
   ],
   "source": [
    "# === 4. False Positive Rate per group\n",
    "def fpr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return fp / (fp + tn) if (fp + tn) > 0 else float(\"nan\")\n",
    "\n",
    "fpr_values = results_df.groupby(\"ethnicity_group\").apply(\n",
    "    lambda g: fpr(g[\"y_true\"], g[\"y_pred\"])\n",
    ")\n",
    "print(\"\\nFalse positive rate (Equalized odds):\\n\", fpr_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "336d2e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision (Predictive parity):\n",
      " ethnicity_group\n",
      "High    0.587815\n",
      "Low     0.566861\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/_41jr63j245210xpt44m7yjh0000gn/T/ipykernel_7075/2254656791.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  prec_values = results_df.groupby(\"ethnicity_group\").apply(\n"
     ]
    }
   ],
   "source": [
    "# === 5. Predictive Parity (Precision per group)\n",
    "prec_values = results_df.groupby(\"ethnicity_group\").apply(\n",
    "    lambda g: precision_score(g[\"y_true\"], g[\"y_pred\"], zero_division=0)\n",
    ")\n",
    "print(\"\\nPrecision (Predictive parity):\\n\", prec_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2d10726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fairness Metrics by Group ===\n",
      "                 Statistical Parity (DP)  Equal Opportunity (TPR)  \\\n",
      "ethnicity_group                                                     \n",
      "High                              0.0416                   0.1118   \n",
      "Low                               0.0329                   0.0920   \n",
      "\n",
      "                 Equalized Odds (FPR)  Predictive Parity (Precision)  \n",
      "ethnicity_group                                                       \n",
      "High                           0.0219                         0.5878  \n",
      "Low                            0.0179                         0.5669  \n",
      "\n",
      "=== Disparities (across groups) ===\n",
      "                               Gap (max-min)  Ratio (min/max)\n",
      "Statistical Parity (DP)               0.0087            0.791\n",
      "Equal Opportunity (TPR)               0.0197            0.823\n",
      "Equalized Odds (FPR)                  0.0041            0.815\n",
      "Predictive Parity (Precision)         0.0210            0.964\n",
      "\n",
      "=== Conditional Statistical Parity ===\n",
      "Weighted gap (max-min across strata): 0.0103\n",
      "Weighted ratio (valid strata only): 0.480\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build summary DataFrame\n",
    "fairness_summary = pd.DataFrame({\n",
    "    \"Statistical Parity (DP)\": dp,\n",
    "    \"Equal Opportunity (TPR)\": tpr_values,\n",
    "    \"Equalized Odds (FPR)\": fpr_values,\n",
    "    \"Predictive Parity (Precision)\": prec_values\n",
    "})\n",
    "\n",
    "# Compute gaps and ratios for each metric\n",
    "gaps = fairness_summary.max() - fairness_summary.min()\n",
    "ratios = fairness_summary.min() / fairness_summary.max()\n",
    "\n",
    "# Nicely formatted output\n",
    "print(\"=== Fairness Metrics by Group ===\")\n",
    "print(fairness_summary.round(4))\n",
    "print(\"\\n=== Disparities (across groups) ===\")\n",
    "print(pd.DataFrame({\"Gap (max-min)\": gaps.round(4), \"Ratio (min/max)\": ratios.round(3)}))\n",
    "\n",
    "# Conditional SP\n",
    "print(\"\\n=== Conditional Statistical Parity ===\")\n",
    "print(f\"Weighted gap (max-min across strata): {w_gap:.4f}\")\n",
    "print(f\"Weighted ratio (valid strata only): {w_ratio:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
